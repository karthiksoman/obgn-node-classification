{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c44665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric.data.data\n",
    "import torch_geometric.data.storage\n",
    "\n",
    "torch.serialization.add_safe_globals([\n",
    "    torch_geometric.data.data.DataEdgeAttr,\n",
    "    torch_geometric.data.data.DataTensorAttr,\n",
    "    torch_geometric.data.storage.GlobalStorage\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958980a",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10472598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features: 0.98 GB\n",
      "Edge index: 1.98 GB\n"
     ]
    }
   ],
   "source": [
    "dataset = PygNodePropPredDataset(name='ogbn-products')\n",
    "data = dataset[0]\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "print(f\"Node features: {data.x.element_size() * data.x.nelement() / 1e9:.2f} GB\")\n",
    "print(f\"Edge index: {data.edge_index.element_size() * data.edge_index.nelement() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79aeb96",
   "metadata": {},
   "source": [
    "## GraphSAGE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d824246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, p=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.dropout_p = p\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336d291",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4a2b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model Hyperparameters\n",
    "NUMBER_OF_LAYERS = 2\n",
    "NEIGHBOR_SAMPLES = [10, 5]\n",
    "HIDDEN_LAYER_DIMENSION = 256\n",
    "\n",
    "# Training Hyperparameters\n",
    "NUMBER_OF_EPOCHS = 25\n",
    "BATCH_SIZE = 1024\n",
    "DROPOUT_P = 0.5\n",
    "LEARNING_RATE = 0.003\n",
    "WORKER_COUNT = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce66c73",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9e2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(data.x.shape[1], HIDDEN_LAYER_DIMENSION, dataset.num_classes, NUMBER_OF_LAYERS, p=DROPOUT_P).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c36b7",
   "metadata": {},
   "source": [
    "## Train and Validation data loader \n",
    "### NeighborLoader keeps the full graph in CPU RAM, but for each iteration, it samples a small subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46844e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=NEIGHBOR_SAMPLES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_nodes=split_idx['train'],\n",
    "    num_workers=WORKER_COUNT,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[-1],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_nodes=split_idx['valid'],\n",
    "    num_workers=WORKER_COUNT,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd949f",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b7035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = total_correct = total_examples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "        y = batch.y[:batch.batch_size].squeeze()\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += float(loss) * batch.batch_size\n",
    "        total_correct += int((out.argmax(dim=-1) == y).sum())\n",
    "        total_examples += batch.batch_size\n",
    "    \n",
    "    return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_correct = total_examples = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "        y = batch.y[:batch.batch_size].squeeze()\n",
    "        total_correct += int((out.argmax(dim=-1) == y).sum())\n",
    "        total_examples += batch.batch_size\n",
    "    return total_correct / total_examples\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a9e4e",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3fdbed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 01: Loss=0.7483, Train=0.8109\n",
      "Epoch 02: Loss=0.4682, Train=0.8745\n",
      "Epoch 03: Loss=0.4375, Train=0.8826\n",
      "Epoch 04: Loss=0.4169, Train=0.8875\n",
      "Epoch 05: Loss=0.4106, Train=0.8898, Val=0.8513\n",
      "Epoch 06: Loss=0.3997, Train=0.8920\n",
      "Epoch 07: Loss=0.4010, Train=0.8913\n",
      "Epoch 08: Loss=0.4148, Train=0.8903\n",
      "Epoch 09: Loss=0.3889, Train=0.8949\n",
      "Epoch 10: Loss=0.3761, Train=0.8977, Val=0.8637\n",
      "Epoch 11: Loss=0.3903, Train=0.8962\n",
      "Epoch 12: Loss=0.3806, Train=0.8984\n",
      "Epoch 13: Loss=0.3725, Train=0.8991\n",
      "Epoch 14: Loss=0.3731, Train=0.8987\n",
      "Epoch 15: Loss=0.3828, Train=0.8971, Val=0.8603\n",
      "Epoch 16: Loss=0.3781, Train=0.8985\n",
      "Epoch 17: Loss=0.3774, Train=0.8980\n",
      "Epoch 18: Loss=0.3615, Train=0.9013\n",
      "Epoch 19: Loss=0.3652, Train=0.9009\n",
      "Epoch 20: Loss=0.3631, Train=0.9016, Val=0.8455\n",
      "Epoch 21: Loss=0.3674, Train=0.9000\n",
      "Epoch 22: Loss=0.3578, Train=0.9021\n",
      "Epoch 23: Loss=0.3584, Train=0.9022\n",
      "Epoch 24: Loss=0.3570, Train=0.9032\n",
      "Epoch 25: Loss=0.4052, Train=0.8954, Val=0.8547\n"
     ]
    }
   ],
   "source": [
    "print(\"Training started...\")\n",
    "for epoch in range(1, NUMBER_OF_EPOCHS + 1):\n",
    "    loss, train_acc = train(epoch)\n",
    "    if epoch % 5 == 0:\n",
    "        val_acc = test(val_loader)\n",
    "        print(f'Epoch {epoch:02d}: Loss={loss:.4f}, Train={train_acc:.4f}, Val={val_acc:.4f}')\n",
    "    else:\n",
    "        print(f'Epoch {epoch:02d}: Loss={loss:.4f}, Train={train_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b25dc1",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8616e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "Final Test Accuracy: 0.6615 (66.15%)\n"
     ]
    }
   ],
   "source": [
    "# Create test loader\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[-1],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_nodes=split_idx['test'],\n",
    "    num_workers=WORKER_COUNT,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_acc = test(test_loader)\n",
    "print(f'Final Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
